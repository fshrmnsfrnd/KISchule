{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!\"C:\\Program Files\\Python310\\python.exe\" -m pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 \n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" \"C:\\Program Files\\Python310\\python.exe\" -m pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaB0B1LdMcPM"
   },
   "source": [
    "## LLMs und Tokenizer laden\n",
    "\n",
    "Die große Frage die sich uns jetzt stellt, wie kommen wir an die LLMs und Tokenizer ran. Oder muss ich sowas erst selbst programmieren?\n",
    "Probieren wir es doch einfach mal aus?\n",
    "\n",
    "### Aufgabe 1:\n",
    "- Googlen sie nach Transformer und AutoModelForCausalLM. Sie werden mit sehr großer Wahrscheinlichkeitt auf der Website Huggingface landen. \n",
    "- Verschaffen sie sich einen Überblick welche Modell(LLM) es hier gibt.\n",
    "- Versuchen sie anschließend unser bisheriges Modell Phi-3-mini... durch das LLM von gpt2 zu ersetzen?\n",
    "- Googlen Sie ggf. nach transformer gpt2.\n",
    "- Auf Huggingface sollten Sie nun finden, wie of das Modell im letzten Monat genutzt wurde, sowie einen Beispiel Code für die \"Installation\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Modelle\n",
    "https://huggingface.co/docs/transformers/model_doc/auto\n",
    "\n",
    "Transformer huggingface gpt2\n",
    "https://huggingface.co/openai-community/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "977271302f104ebdb11c0a2a8183cc93",
      "931c6ca8360a4e79924399f017f5522d",
      "2ff276f9bbab440bb036c1109c023582",
      "6d5243b8997d4f64af62900cb19489d0",
      "680d3647069b45c0b4353548ac52ef37",
      "0512bf845dda462395f58c06dfb0733d",
      "26985bad36884566a7bd4ba6f1037e80",
      "e174502c4dfe4740922264d67a9a74b5",
      "9da5552b33c1418d9e0190ec66f19c9d",
      "33acfbdb950545a6a67a9b39be02e30b",
      "c6d9399532c14ce58189b9d038e4cfa4"
     ]
    },
    "executionInfo": {
     "elapsed": 58517,
     "status": "ok",
     "timestamp": 1715083822638,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "mNkbw28oMeAM",
    "outputId": "c0b97b59-180f-427f-8d49-398cb1b39c40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n.natzer\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, pipeline, AutoTokenizer, set_seed\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'gpt2',\n",
    "    device_map='auto',\n",
    "    dtype='auto',\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "#generator = pipeline('text-generation', model='gpt2')\n",
    "#set_seed(42)\n",
    "#tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol5aCUfM9bjM"
   },
   "source": [
    "### Aufgabe 2:\n",
    "- Versuchen Sie nun einen beliebigen Prompt auf das neue LLM anzuwenden.\n",
    "- Vermutlich bekommen Sie eine Fehlermeldung. Wenn dies geschieht können Sie am Ende der Fehlermeldung sich durch Gemini den Fehler erklären und korrigieren lassen. Probieren Sie es doch einfach aus. \n",
    "- (Der Fehler liegt höchstwahrscheinlich am String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1715085868022,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "Ne92BtrXU8k8",
    "outputId": "ce08f1ae-8fb4-4c39-cb18-8d975f5ced31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=50) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'We live in a society that is not perfect,\" he says. \"But it is not the only problem and is not one that we have to address.\"\\n\\nHe says that \"when you get to the point where you think, \\'What is the problem?\\' you cannot'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"We live in a society\", max_length=50, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cugB9temhHJ0"
   },
   "source": [
    "### Aufgabe 3:\n",
    "Was passiert wenn Sie LLM-Model und Tokenizer mischen? Falls der Fehler noch nicht aufgetreten ist Probieren Sie es doch mal aus!\n",
    "Verwenden Sie für das LLM gpt2 und für den Tokenizer das alte Phi-3-... Modell. Und wenden Sie anschließend einen Prompt hierauf an. \n",
    "Funktioniert alles wie gewünscht?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3336,
     "status": "ok",
     "timestamp": 1715086284771,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "VXYuFn9eG2t4",
    "outputId": "3f96c196-8d37-44ca-fad5-94ddcbe8fc57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8805,
     "status": "ok",
     "timestamp": 1715086322608,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "fNYi3eDRG9Sk",
    "outputId": "0e1f5a80-27a4-42ef-b514-36e54dea708c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Create a funny joke about chickens.<|end|>\n",
      "<|endoftext|>\n",
      "O minorlin Iominveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldomveinorldom\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jVOOib72Z0P"
   },
   "source": [
    "## Erklärung\n",
    "\n",
    "Tokenizer und LLM sind untrennbar mit einander verknüpft. Dadurch das der Tokenizer mit den Informationen aus dem LLM trainiert wurde, können Tokenizer und LLMs nicht gemischt werden. (Die Zuordnung der Vektoren stimmt einfach nicht mehr.)\n",
    "\n",
    "Um alle Modell auf Huggingface nutzen zu können ist eine Anmeldung notwendig. Diese brauchen sie jedoch nicht für den Unterricht. Wir beschränken uns auf die Modell die frei Verfügbar sind. Sollten Sie diese jedoch später nutzen wollen empfiehlt sich eine Anmeldung. Huggingface bietet über 60.000 Modelle. Welche davon für ihren Anwendungsfall sinnvoll wäre, müssen sie dann testen."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMRJzDTVTlBntcudlyKW5VZ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
